{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2_w2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng lại bài toán t-SNE. Tính đạo hàm loss với các parameter (y) trong bài toán t-SNE\n"
      ],
      "metadata": {
        "id": "sm5VxCRT1b22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The similarity of data point $x_j$ to data point $x_i$ is the conditional probability, $~p_{(j|i)}$. \\newline\n",
        "For nearby data points, $p_{(j|i)}$ is relatively high, whereas for widely separated data points, $p_{(j|i)}$ will be almost infinitesimal \\newline\n",
        "The conditional probability $p_{(j|i)}$ is given by: \n",
        "$$p_{(j|i)} = \\frac{e^{-\\left \\| x_i - x_j \\right \\|^2/ 2 \\sigma_i^2}}{\\sum_{k\\neq i} e^{-\\left \\| x_i - x_k \\right \\|^2/ 2 \\sigma_i^2}}$$\n",
        "where $\\sigma_i$ is the variance of the Gaussian that is centered on data point $x_i$\n",
        "For the low-dimensional counterparts $y_i$ and $y_j$ of the high-dimensional data points $x_i$ and $x_j$, it is possible to compute a similar conditional probability, which we denote by $q_{(j|i)}$:\n",
        "$$q_{(j|i)} = \\frac{e^{-\\left \\| y_i - y_j \\right \\|^2}}{\\sum_{k\\neq i}e^{-\\left \\| y_i - y_k \\right \\|^2}}$$\n",
        "Since we are only interested in modeling pairwise similarities, we set $q_{(i|i)} = 0$. \\newline\n",
        "If the map points $y_i$ and $y_j$ correctly model the similarity between the high-dimensional data points $x_i$ and $x_j$, the conditional probabilities $p_{j|i}$ and $q_{j|i}$ will be equal \\newline\n",
        "SNE minimizes the sum of Kullback-Leibler divergences over all data points using a gradient descent method. The cost function C is given: \\newline\n",
        "$$C = \\sum_{i} KL(P_i||Q_i) = \\sum_{i} \\sum_{j} p_{(j|i)} log \\frac{p_{j|i}}{q_{j|i}}$$\n",
        "in which $P_i$ represents the conditional probability distribution over all other data points given data point $x_i$, and $Q_i$ represents the conditional probability distribution over all other map points given map point $y_i$ \\newline\n",
        "\n",
        "Because the Kullback-Leibler divergence is not symmetric, different types of error\n",
        "in the pairwise distances in the low-dimensional map are not weighted equal \\newline\n",
        "\n",
        "SNE performs a binary search for the value of $\\sigma_i$ that produces a $P_i$ with a fixed perplexity that is specified by the user. The perplexity is defined as: \n",
        "$$Perp(P_i) = 2^{H(P_i)}$$\n",
        "where $H(P_i)$ is the Shannon entropy of $P_i$ measured in bits\n",
        "$$H(P_i) = -\\sum_{j} p_{(j|i)} log_{2}p_{(j|i)}$$\n",
        "The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50. \\newline\n",
        "The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form: \n",
        "$$\\frac{\\delta C}{\\delta y_i} = 2 \\sum_{j} (p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j})(y_i - y_j) $$\n",
        "Mathematically, the gradient update with a momentum term is given by:\n",
        "$$\\gamma^{(t)} = \\gamma^{(t-1)} + \\eta \\frac{\\delta C}{\\delta y} + \\alpha (t)(\\gamma^{(t-1)} - \\gamma^{(t-2)}) $$\n",
        "where $\\gamma^{(t)}$ indicates the solution at iteration t, $\\eta$ indicates the learning rate, and $\\alpha(t)$ represents the momentum at iteration t. \\newline\n",
        "\n",
        "As an alternative to minimizing the sum of the Kullback-Leibler divergences between the conditional probabilities $p_{j|i}$ and $q_{j|i}$ it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution, P, in the high-dimensional space and a joint probability distribution, Q, in the low-dimensional space:\n",
        "$$C = \\sum_{i} KL(P||Q) = \\sum_{i} \\sum_{j} p_{(j|i)} log \\frac{p_{ij}}{q_{ij}}$$\n",
        "Set $p_{ii}$ and $q_{ii}$ to zero. We refer to this type of SNE as symmetric SNE, because it\n",
        "has the property that $p_{ij}= p_{ji}$ and $q_{ij}= q_{ji} \\forall i,j$ \\newline\n",
        "In symmetric SNE, the pairwise similarities in the low-dimensional map $q_{ij}$ are given by:\n",
        "$$q_{ij} = \\frac{e^{-\\left \\| y_i - y_j \\right \\|^2}}{\\sum_{k\\neq l}e^{-\\left \\| y_k - y_l \\right \\|^2}}$$\n",
        "The obvious way to define the pairwise similarities in the high-dimensional space $p_{(ij)}$ is:\n",
        "$$p_{ij} = \\frac{e^{-\\left \\| x_i - x_j \\right \\|^2/ 2 \\sigma^2}}{\\sum_{k\\neq l} e^{-\\left \\| x_k - x_l \\right \\|^2/ 2 \\sigma^2}}$$\n",
        "The gradient of symmetric SNE is fairly similar to that of asymmetric SNE, and is given:\n",
        "$$\\frac{\\delta C}{\\delta y_i} = 4 \\sum_{j} (p_{ij} - q_{ij})(y_i - y_j)$$"
      ],
      "metadata": {
        "id": "GDnXWZKP1_Aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dùng t-SNE (sklearn) giảm chiều dữ liệu MNIST về 2 chiều, so sánh chuyện giảm chiều PCA và t-SNE.\n"
      ],
      "metadata": {
        "id": "GJ439w5d1gia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YZszd7372bV2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "X = mnist.data\n",
        "y = mnist.target"
      ],
      "metadata": {
        "id": "3nQh-A7i2flG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "BmjSwZFR2j4j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedded = TSNE(\n",
        "        n_components=2,\n",
        "        random_state=0).fit_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URM76Tib2mJd",
        "outputId": "05e2ec86-eadd-4215-8699-8ce4f74a0123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(x = X_embedded[:,0], y = X_embedded[:,1], hue = y, palette = sns.hls_palette(10), legend = 'full');"
      ],
      "metadata": {
        "id": "HSQNunBC21_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_X = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "Hxfk_5O3236p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(x = pca_X[:,0], y = pca_X[:,1], hue = y, palette = sns.hls_palette(10), legend = 'full')"
      ],
      "metadata": {
        "id": "s0UFphUw26NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional, cộng điểm) tự implement thuật toán t-SNE để giảm chiều dữ liệu IRIS về 2 chiều."
      ],
      "metadata": {
        "id": "jg2TtCIF1h5Y"
      }
    }
  ]
}